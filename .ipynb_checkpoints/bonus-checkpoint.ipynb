{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 -- Beat the Benchmark (bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The libraries that we are going to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt1\n",
    "import matplotlib.pyplot as plt2\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "import operator\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "from wordcloud import STOPWORDS\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, auc\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our data, we create our assistant-structures and we define our stopwords, our vectorizer *(count vectorizer)* and our category criterion, the *title*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load our data\n",
    "test_data = pd.read_csv('test_set.csv', sep='\\t')\n",
    "train_data = pd.read_csv('train_set.csv', sep='\\t')\n",
    "\n",
    "# a list of our categories (taken as facts)\n",
    "categories = ['Politics','Football','Business','Technology','Film']\n",
    "\n",
    "# we will use a number to represent each of our categories\n",
    "category_dict = {'Politics':0, 'Football':1, 'Business':2, 'Technology':3, 'Film':4}\n",
    "\n",
    "# for our text data, we use a count vectorizer\n",
    "stopwords = set(STOPWORDS) | set(ENGLISH_STOP_WORDS)\n",
    "# some additional stopwords based on our own observations\n",
    "stopwords.add('said')\n",
    "stopwords.add('say')\n",
    "stopwords.add('says')\n",
    "stopwords.add('set')\n",
    "\n",
    "# our count vectorizer\n",
    "count_vect = CountVectorizer(stop_words=stopwords)\n",
    "\n",
    "# we will classify using the 'Title' as a criterion\n",
    "category_criterion = 'Title'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess our training and testing data. We then create a *'target'* array where we will note the category of each of our training data and we print a small part of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12266, 13712)\n",
      "(12266, 13712)\n",
      "(3067, 13712)\n",
      "(3067, 13712)\n",
      "target[] sample:\n",
      "[2 2 2 1 1 2 0 1 2 4 2 4 4 4 2 4 0 2 0 0 1 3 0 2 4 1 0 4 2 3 1 0 0 2 1 3 2\n",
      " 0 3 3]\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING\n",
    "# for training\n",
    "X_train_counts = count_vect.fit_transform(train_data[category_criterion])\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_counts.shape)\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "# for testing\n",
    "X_test_counts = count_vect.transform(test_data[category_criterion])\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "print(X_test_counts.shape)\n",
    "print(X_test_tfidf.shape)\n",
    "\n",
    "# we create a 'target' array where we will note the category of each of our training data\n",
    "target = []\n",
    "for x in train_data['Category']:\n",
    "    target.append(category_dict[x])\n",
    "\n",
    "target = np.array(target)\n",
    "print(\"target[] sample:\")\n",
    "print(target[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will use the Decision Tree Classifier to Beat the Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By experimenting with various classifiers in Part 3 we ended up choosing the *Random Forest Classifier* as we saw that it produces better evaluation metrics than all the others. For preproccesing we used the *Count Vectorizer* excluding the stopwords that we have setted since Part 1 of this project, we also used the *TfidfTransformer (term-frequency times inverse document-frequency transformer)* because we observed that increases the accuracy of our classifier more than anything else that we tried. We also observed that *Decision Tree Classifier* under the conditions described above can Beat the Random Forest Classifier, which can be proved by the following results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RANDOM FOREST (RF) CLASSIFIER\n",
    "RANDOM_STATE = 123\n",
    "\n",
    "rndf = RandomForestClassifier(warm_start=True, oob_score=True, max_features=\"sqrt\", random_state=RANDOM_STATE)\n",
    "rndf.set_params(n_estimators=30)\n",
    "rndf.fit(X_train_tfidf, target)\n",
    "\n",
    "predicted = rndf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We experiment with the Latent Semantic Indexing (LSI) for various number of components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent Semantic Indexing (LSI) for various number of components: \n",
      "('For ', 100, ' components:')\n",
      "Accuracy:\n",
      "('acc == ', 0.5131257133539866)\n",
      "0.513125713354\n",
      "('For ', 200, ' components:')\n",
      "Accuracy:\n",
      "('acc == ', 0.47578672753954021)\n",
      "0.47578672754\n",
      "('For ', 300, ' components:')\n",
      "Accuracy:\n",
      "('acc == ', 0.43665416598728191)\n",
      "0.436654165987\n",
      "('For ', 400, ' components:')\n",
      "Accuracy:\n",
      "('acc == ', 0.44040436980270664)\n",
      "0.440404369803\n",
      "('For ', 500, ' components:')\n",
      "Accuracy:\n",
      "('acc == ', 0.38529267894994296)\n",
      "0.38529267895\n",
      "('For ', 600, ' components:')\n",
      "Accuracy:\n",
      "('acc == ', 0.40200554377955322)\n",
      "0.40200554378\n"
     ]
    }
   ],
   "source": [
    "print(\"Latent Semantic Indexing (LSI) for various number of components: \")\n",
    "\n",
    "accuracy = []\n",
    "components = []\n",
    "for i in range(6):\n",
    "    components.append(i*100+100)\n",
    "    print(\"For \", components[i], \" components:\")\n",
    "    rndf = RandomForestClassifier(warm_start=True, oob_score=True, max_features=\"sqrt\", random_state=RANDOM_STATE)\n",
    "\n",
    "    svd = TruncatedSVD(n_components=components[i])\n",
    "    X_lsi = svd.fit_transform(X_train_tfidf)\n",
    "    clfSVD = tree.DecisionTreeClassifier().fit(X_lsi, target)\n",
    "    X_test_lsi = svd.transform(X_train_counts)\n",
    "    predictedSVD = clfSVD.predict(X_test_lsi)\n",
    "    print(\"Accuracy:\")\n",
    "    acc = accuracy_score(target, predictedSVD)\n",
    "    accuracy.append(acc)\n",
    "    print(\"acc == \",acc)\n",
    "    print(accuracy[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00115585,  0.00079303,  0.00109195, ..., -0.02479855,\n",
       "        -0.0298099 , -0.02092812],\n",
       "       [ 0.0029537 ,  0.0036768 ,  0.01050912, ...,  0.02084935,\n",
       "        -0.00964101,  0.01041619],\n",
       "       [ 0.00362246,  0.0031201 ,  0.00949418, ...,  0.07918788,\n",
       "        -0.00520475, -0.03225572],\n",
       "       ..., \n",
       "       [ 0.00215124,  0.00457309,  0.02670653, ..., -0.02355548,\n",
       "        -0.0115988 ,  0.00904519],\n",
       "       [ 0.01173553,  0.02352226,  0.11453489, ..., -0.00206927,\n",
       "        -0.00839041, -0.01279061],\n",
       "       [ 0.00057599,  0.00324241,  0.00214254, ..., -0.00492281,\n",
       "        -0.00052916, -0.03024586]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DECISION TREE (DT) CLASSIFIER\n",
    "dt_clf = tree.DecisionTreeClassifier()\n",
    "dt_clf.fit(X_lsi, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our Cross Validation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_val_instance = 0\n",
    "\n",
    "def cross_validate(clf):\n",
    "    global cross_val_instance    # Needed to modify global copy of a global variable\n",
    "    \n",
    "    kf = KFold(n_splits=10)\n",
    "\n",
    "    fold = 0\n",
    "    for train_index, test_index in kf.split(train_data[category_criterion]):\n",
    "        cross_val_instance += 1\n",
    "        \n",
    "        X_train_counts = count_vect.transform(train_data[category_criterion][train_index])\n",
    "        X_test_counts = count_vect.transform(train_data[category_criterion][test_index].values.astype('U'))\n",
    "\n",
    "        tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "        X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "        \n",
    "        clf_cv = clf.fit(X_train_tfidf, target[train_index])\n",
    "        X_test_tfidf = tfidf_transformer.fit_transform(X_test_counts)\n",
    "        \n",
    "        yPred = clf_cv.predict(X_test_tfidf)\n",
    "        fold += 1\n",
    "        print (\"Fold \" + str(fold))\n",
    "        \n",
    "        accuracy = accuracy_score(target[test_index], yPred)\n",
    "        print(\"Accuracy: \", accuracy)\n",
    "        \n",
    "        A = auc(target[test_index], yPred, reorder=True)\n",
    "        print(\"AUC: \", A)\n",
    "        \n",
    "        p = metrics.precision_score(target[test_index], yPred, average='macro')\n",
    "        print(\"PRESICION: \", p)\n",
    "        \n",
    "        recall = metrics.recall_score(target[test_index], yPred, average='macro') \n",
    "        print(\"Recall: \", recall)\n",
    "        f_1 = metrics.f1_score(target[test_index], yPred, average='micro') \n",
    "        print(\"F-1: \", f_1)\n",
    "        \n",
    "        fpr, tpr, thresholds = metrics.roc_curve(target[test_index], yPred, pos_label=2)\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "        print(\"Roc: \",roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "('Accuracy: ', 0.72697636511817443)\n",
      "('AUC: ', 8.0)\n",
      "('PRESICION: ', 0.7965741169240802)\n",
      "('Recall: ', 0.73688816792454226)\n",
      "('F-1: ', 0.72697636511817443)\n",
      "('Roc: ', 0.50337887130339953)\n",
      "Fold 2\n",
      "('Accuracy: ', 0.74001629991850038)\n",
      "('AUC: ', 8.5)\n",
      "('PRESICION: ', 0.78739513224066682)\n",
      "('Recall: ', 0.72265702888536099)\n",
      "('F-1: ', 0.74001629991850038)\n",
      "('Roc: ', 0.60895853335162065)\n",
      "Fold 3\n",
      "('Accuracy: ', 0.74246128769356157)\n",
      "('AUC: ', 8.5)\n",
      "('PRESICION: ', 0.77861927041124768)\n",
      "('Recall: ', 0.72269671322491214)\n",
      "('F-1: ', 0.74246128769356157)\n",
      "('Roc: ', 0.60498039995690478)\n",
      "Fold 4\n",
      "('Accuracy: ', 0.73594132029339854)\n",
      "('AUC: ', 8.0)\n",
      "('PRESICION: ', 0.78411216864538336)\n",
      "('Recall: ', 0.72391124527797435)\n",
      "('F-1: ', 0.73594132029339865)\n",
      "('Roc: ', 0.59531980690903608)\n",
      "Fold 5\n",
      "('Accuracy: ', 0.7351263243683781)\n",
      "('AUC: ', 8.0)\n",
      "('PRESICION: ', 0.78027521344946427)\n",
      "('Recall: ', 0.71432202938595601)\n",
      "('F-1: ', 0.73512632436837799)\n",
      "('Roc: ', 0.61477221300346963)\n",
      "Fold 6\n",
      "('Accuracy: ', 0.73105134474327627)\n",
      "('AUC: ', 8.0)\n",
      "('PRESICION: ', 0.78090756945508111)\n",
      "('Recall: ', 0.71169744230727194)\n",
      "('F-1: ', 0.73105134474327638)\n",
      "('Roc: ', 0.61799024730059215)\n",
      "Fold 7\n",
      "('Accuracy: ', 0.73980424143556278)\n",
      "('AUC: ', 8.0)\n",
      "('PRESICION: ', 0.78681272649966327)\n",
      "('Recall: ', 0.72464077879895983)\n",
      "('F-1: ', 0.73980424143556278)\n",
      "('Roc: ', 0.59444585220613855)\n",
      "Fold 8\n",
      "('Accuracy: ', 0.75367047308319735)\n",
      "('AUC: ', 8.0)\n",
      "('PRESICION: ', 0.79124658398859127)\n",
      "('Recall: ', 0.73564597500232443)\n",
      "('F-1: ', 0.75367047308319735)\n",
      "('Roc: ', 0.62174044605827594)\n",
      "Fold 9\n",
      "('Accuracy: ', 0.74551386623164762)\n",
      "('AUC: ', 8.0)\n",
      "('PRESICION: ', 0.79765900171002868)\n",
      "('Recall: ', 0.73092624492624503)\n",
      "('F-1: ', 0.74551386623164762)\n",
      "('Roc: ', 0.61372335340789608)\n",
      "Fold 10\n",
      "('Accuracy: ', 0.74469820554649269)\n",
      "('AUC: ', 8.0)\n",
      "('PRESICION: ', 0.78163349350580336)\n",
      "('Recall: ', 0.71965259006335836)\n",
      "('F-1: ', 0.74469820554649269)\n",
      "('Roc: ', 0.6384063502944568)\n"
     ]
    }
   ],
   "source": [
    "cross_validate(dt_clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
